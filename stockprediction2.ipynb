{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogReturns(dataList):\n",
    "    logReturns = []\n",
    "    for t in range(len(dataList)-1):\n",
    "        logReturn = np.log(dataList[t+1]) - np.log(dataList[t])\n",
    "        logReturns.append(logReturn)\n",
    "    return logReturns\n",
    "\n",
    "def pricesToLogReturns(data):\n",
    "    dataT = data.T\n",
    "    logReturns = []\n",
    "    for stock in dataT.values:\n",
    "        logReturns.append(getLogReturns(stock))\n",
    "    return pd.DataFrame(logReturns).T\n",
    "\n",
    "def getPrincipleComponents(componentCount,inputData):\n",
    "    pca = PCA(n_components=componentCount)\n",
    "    principalComponents = pca.fit_transform(inputData)\n",
    "    principalDf = pd.DataFrame(data = principalComponents)\n",
    "    return principalDf\n",
    "\n",
    "def plotPCData(pcData):\n",
    "    formatedData = pcData.T\n",
    "    m,n = formatedData.shape\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    for i in range(m):\n",
    "        ax.plot(formatedData.iloc[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# Import data\n",
    "num_comp = 5 # Choose number of PCA components\n",
    "raw = pd.read_csv('data_stocks.csv')\n",
    "testSet = (raw.iloc[0:1000,1:]) # Get rid of date column\n",
    "testSet = pricesToLogReturns(testSet)\n",
    "testSet = testSet.values\n",
    "temp1 = getPrincipleComponents(num_comp,testSet) # Calculate PCA components\n",
    "\n",
    "# Combine first column of raw data (S&P 500) with PCA components\n",
    "data = np.zeros((temp1.shape[0],num_comp+1))\n",
    "data[:,0] = testSet[:,0]\n",
    "data[:,1:] = temp1 \n",
    "\n",
    "fig3 = plt.figure()\n",
    "ax3 = fig3.add_subplot(111)\n",
    "plotPCData(temp1)\n",
    "plt.title(\"PCA Components vs. Time\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.show()\n",
    "\n",
    "# Dimensions of dataset\n",
    "n = data.shape[0]\n",
    "p = data.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test data\n",
    "train_start = 0\n",
    "train_end = int(np.floor(0.7*n))\n",
    "test_start = train_end + 1\n",
    "test_end = int(np.floor(0.9*n))\n",
    "valid_start = test_end + 1\n",
    "valid_end = n\n",
    "\n",
    "data_train = data[np.arange(train_start, train_end), :]\n",
    "data_test = data[np.arange(test_start, test_end), :]\n",
    "data_valid = data[np.arange(valid_start, valid_end), :]\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build X and y\n",
    "X_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0]\n",
    "X_test = data_test[:, 1:]\n",
    "y_test = data_test[:, 0]\n",
    "X_valid = data_valid[:, 1:]\n",
    "y_valid = data_valid[:, 0]\n",
    "\n",
    "# Number of stocks in training data\n",
    "n_stocks = X_train.shape[1]\n",
    "\n",
    "# Neurons\n",
    "n_neurons_1 = 1024\n",
    "n_neurons_2 = 512\n",
    "n_neurons_3 = 256\n",
    "n_neurons_4 = 128\n",
    "\n",
    "# Session\n",
    "net = tf.Session()\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "# Initializers\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Hidden weights\n",
    "W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n",
    "\n",
    "# Output weights\n",
    "W_out = tf.Variable(weight_initializer([n_neurons_4, 1]))\n",
    "bias_out = tf.Variable(bias_initializer([1]))\n",
    "\n",
    "# Hidden layer\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# Output layer (transpose!)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))\n",
    "\n",
    "# Cost function\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.train.AdamOptimizer().minimize(mse)\n",
    "\n",
    "# Init\n",
    "net.run(tf.global_variables_initializer())\n",
    "\n",
    "# Setup plot\n",
    "plt.ion()\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "line1, = ax1.plot(y_test,label=\"Actual Test Data\")\n",
    "line2, = ax1.plot(y_test * 0.5,label=\"Predicted Test Data\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Fit neural net\n",
    "batch_size = 256\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "mse_valid = []\n",
    "\n",
    "# Run\n",
    "epochs = 100\n",
    "counter = 0\n",
    "for e in range(epochs):\n",
    "\n",
    "    # Shuffle training data\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    X_train = X_train[shuffle_indices]\n",
    "    y_train = y_train[shuffle_indices]\n",
    "\n",
    "    # Minibatch training\n",
    "    for i in range(0, len(y_train) // batch_size):\n",
    "        start = i * batch_size\n",
    "        batch_x = X_train[start:start + batch_size]\n",
    "        batch_y = y_train[start:start + batch_size]\n",
    "        # Run optimizer with batch\n",
    "        net.run(opt, feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        # Show progress\n",
    "        if np.mod(i, 50) == 0:\n",
    "            counter = counter + 1\n",
    "            # MSE train and test\n",
    "            mse_train.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))\n",
    "            mse_test.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))\n",
    "            # Prediction\n",
    "            pred = net.run(out, feed_dict={X: X_test})\n",
    "            \n",
    "            line2.set_ydata(pred)\n",
    "            plt.title('Epoch ' + str(e) + ', Batch ' + str(i))\n",
    "            plt.show()\n",
    "            \n",
    "            if counter == 1:\n",
    "                initial_pred = pred\n",
    "            if counter == epochs:\n",
    "                final_pred_valid = y_valid_pred\n",
    "                final_pred = pred\n",
    "            \n",
    "            plt.pause(0.01)\n",
    "#             if counter == 1:\n",
    "#                 plt.pause(10)\n",
    "#             else:\n",
    "#                 plt.pause(0.01)\n",
    "            \n",
    "plt.ioff()\n",
    "            \n",
    "fig4 = plt.figure()\n",
    "ax4 = fig4.add_subplot(111)\n",
    "line1, = ax4.plot(y_test,label=\"Actual Test Data\")\n",
    "line2, = ax4.plot(y_test * 0.5,label=\"Predicted Test Data\")\n",
    "line2.set_ydata(initial_pred)\n",
    "plt.title(\"Predicted Test Data vs. Actual Test Data (Initial Plot)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "            \n",
    "fig5 = plt.figure()\n",
    "ax5 = fig5.add_subplot(111)\n",
    "line1, = ax5.plot(y_test,label=\"Actual Test Data\")\n",
    "line2, = ax5.plot(y_test * 0.5,label=\"Predicted Test Data\")\n",
    "line2.set_ydata(final_pred)\n",
    "plt.title(\"Predicted Test Data vs. Actual Test Data (Final Plot)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "line1, = ax2.plot(mse_train,label=\"Training Error\")\n",
    "line2, = ax2.plot(mse_test,label=\"Testing Error\")\n",
    "plt.title(\"Training and Testing Error\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on all data (500 stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "\n",
    "# Import data\n",
    "raw = pd.read_csv('data_stocks.csv')\n",
    "testSet = (raw.iloc[0:1000,1:])\n",
    "data = pricesToLogReturns(testSet)\n",
    "\n",
    "fig3 = plt.figure()\n",
    "ax3 = fig3.add_subplot(111)\n",
    "plotPCData(data)\n",
    "plt.title(\"All 500 Stocks vs. Time\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.show()\n",
    "\n",
    "# Dimensions of dataset\n",
    "n = data.shape[0]\n",
    "p = data.shape[1]\n",
    "\n",
    "data = data.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test data\n",
    "train_start = 0\n",
    "train_end = int(np.floor(0.7*n))\n",
    "test_start = train_end + 1\n",
    "test_end = int(np.floor(0.9*n))\n",
    "valid_start = test_end + 1\n",
    "valid_end = n\n",
    "\n",
    "data_train = data[np.arange(train_start, train_end), :]\n",
    "data_test = data[np.arange(test_start, test_end), :]\n",
    "data_valid = data[np.arange(valid_start, valid_end), :]\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build X and y\n",
    "X_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0]\n",
    "X_test = data_test[:, 1:]\n",
    "y_test = data_test[:, 0]\n",
    "X_valid = data_valid[:, 1:]\n",
    "y_valid = data_valid[:, 0]\n",
    "\n",
    "# Number of stocks in training data\n",
    "n_stocks = X_train.shape[1]\n",
    "\n",
    "# Neurons\n",
    "n_neurons_1 = 1024\n",
    "n_neurons_2 = 512\n",
    "n_neurons_3 = 256\n",
    "n_neurons_4 = 128\n",
    "\n",
    "# Session\n",
    "net = tf.Session()\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "# Initializers\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()\n",
    "\n",
    "# Hidden weights\n",
    "W_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\n",
    "bias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\n",
    "W_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\n",
    "bias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\n",
    "W_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\n",
    "bias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\n",
    "W_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\n",
    "bias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n",
    "\n",
    "# Output weights\n",
    "W_out = tf.Variable(weight_initializer([n_neurons_4, 1]))\n",
    "bias_out = tf.Variable(bias_initializer([1]))\n",
    "\n",
    "# Hidden layer\n",
    "hidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\n",
    "hidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\n",
    "hidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\n",
    "hidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n",
    "\n",
    "# Output layer (transpose!)\n",
    "out = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))\n",
    "\n",
    "# Cost function\n",
    "mse = tf.reduce_mean(tf.squared_difference(out, Y))\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.train.AdamOptimizer().minimize(mse)\n",
    "\n",
    "# Init\n",
    "net.run(tf.global_variables_initializer())\n",
    "\n",
    "# Setup plot\n",
    "plt.ion()\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "line1, = ax1.plot(y_test,label=\"Actual Test Data\")\n",
    "line2, = ax1.plot(y_test * 0.5,label=\"Predicted Test Data\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Fit neural net\n",
    "batch_size = 256\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "mse_valid = []\n",
    "\n",
    "# Run\n",
    "epochs = 100\n",
    "counter = 0\n",
    "for e in range(epochs):\n",
    "\n",
    "    # Shuffle training data\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "    X_train = X_train[shuffle_indices]\n",
    "    y_train = y_train[shuffle_indices]\n",
    "\n",
    "    # Minibatch training\n",
    "    for i in range(0, len(y_train) // batch_size):\n",
    "        start = i * batch_size\n",
    "        batch_x = X_train[start:start + batch_size]\n",
    "        batch_y = y_train[start:start + batch_size]\n",
    "        # Run optimizer with batch\n",
    "        net.run(opt, feed_dict={X: batch_x, Y: batch_y})\n",
    "        \n",
    "        # Show progress\n",
    "        if np.mod(i, 50) == 0:\n",
    "            counter = counter + 1\n",
    "            # MSE train and test\n",
    "            mse_train.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))\n",
    "            mse_test.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))\n",
    "            # Prediction\n",
    "            pred = net.run(out, feed_dict={X: X_test})\n",
    "            \n",
    "            line2.set_ydata(pred)\n",
    "            plt.title('Epoch ' + str(e) + ', Batch ' + str(i))\n",
    "            plt.show()\n",
    "            \n",
    "            if counter == 1:\n",
    "                initial_pred = pred\n",
    "            if counter == epochs:\n",
    "                final_pred_valid = y_valid_pred\n",
    "                final_pred = pred\n",
    "            \n",
    "            plt.pause(0.01)\n",
    "#             if counter == 1:\n",
    "#                 plt.pause(10)\n",
    "#             else:\n",
    "#                 plt.pause(0.01)\n",
    "            \n",
    "plt.ioff()\n",
    "            \n",
    "fig4 = plt.figure()\n",
    "ax4 = fig4.add_subplot(111)\n",
    "line1, = ax4.plot(y_test,label=\"Actual Test Data\")\n",
    "line2, = ax4.plot(y_test * 0.5,label=\"Predicted Test Data\")\n",
    "line2.set_ydata(initial_pred)\n",
    "plt.title(\"Predicted Test Data vs. Actual Test Data (Initial Plot)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "            \n",
    "fig5 = plt.figure()\n",
    "ax5 = fig5.add_subplot(111)\n",
    "line1, = ax5.plot(y_test,label=\"Actual Test Data\")\n",
    "line2, = ax5.plot(y_test * 0.5,label=\"Predicted Test Data\")\n",
    "line2.set_ydata(final_pred)\n",
    "plt.title(\"Predicted Test Data vs. Actual Test Data (Final Plot)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Return Value\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "line1, = ax2.plot(mse_train,label=\"Training Error\")\n",
    "line2, = ax2.plot(mse_test,label=\"Testing Error\")\n",
    "plt.title(\"Training and Testing Error\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
